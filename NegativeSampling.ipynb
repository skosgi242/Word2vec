{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import time \n",
    "from sklearn import preprocessing as pre\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from Stemmer import Stemmer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict = dict()\n",
    "lemmatize_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"/Users/skosgi/Downloads/nlp/\"\n",
    "\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "index_path = \"/Users/skosgi/Downloads/nlp/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    punctuations = '0123456789'\n",
    "    \n",
    "    d = str.maketrans(dict.fromkeys(punctuations,\" \"))\n",
    "    data = data.translate(d)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(token):\n",
    "    stemmer = Stemmer('english')\n",
    "    if token in stem_dict:\n",
    "        token = stem_dict[token]\n",
    "    else:\n",
    "        stem_word = stemmer.stemWord(token)\n",
    "        stem_dict[token] = stem_word\n",
    "        token = stem_word\n",
    "    if token in lemmatize_dict:\n",
    "        token = lemmatize_dict[token]\n",
    "    else:\n",
    "        lem_word = lemmatizer.lemmatize(token)\n",
    "        lemmatize_dict[token] = lem_word\n",
    "        token = lem_word\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_set 4940\n",
      "13.150115966796875\n"
     ]
    }
   ],
   "source": [
    "#code to generate dictionary of words, this also generates Negative sampling table(final_list).\n",
    "t = time.time()\n",
    "token_set = set()\n",
    "count = 0;\n",
    "stop_words = stopwords.words('english')\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "freq_map = {}\n",
    "for line in infile:\n",
    "    count += 1\n",
    "    if count == review_count:\n",
    "        break\n",
    "    jsondict = json.loads(line)\n",
    "    review_text = clean(jsondict[\"reviewText\"].lower())\n",
    "    tokens = tokenizer.tokenize(review_text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) <=2:\n",
    "            continue\n",
    "        token = preprocess(token)\n",
    "        token_set.add(token)\n",
    "        if token in freq_map.keys():\n",
    "            freq_map[token] = freq_map[token]+1\n",
    "        else:\n",
    "            freq_map[token] = 1\n",
    "token_set = list(token_set)\n",
    "token_set = [k for k in freq_map.keys() if freq_map[k]>=5]        \n",
    "print(\"token_set\",len(token_set))\n",
    "\n",
    "infile.close()\n",
    "enc_map = {}\n",
    "for i in range(len(token_set)):\n",
    "    enc_map[token_set[i]] = i\n",
    "final_list = []\n",
    "total_prob=0\n",
    "table_size = 100000000\n",
    "for key in token_set:\n",
    "    k = freq_map[key]\n",
    "    k = np.power(k,0.75)\n",
    "    total_prob +=k\n",
    "for key in token_set:\n",
    "    k = freq_map[key]\n",
    "    k = np.power(k,0.75)\n",
    "    pr_k = table_size*k/total_prob\n",
    "    for i in range(int(pr_k)):\n",
    "        final_list.append(key)\n",
    "        \n",
    "print(time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickfile = open(\"dictionary_mappingsg2\",\"wb\")\n",
    "pickle.dump(enc_map,pickfile)\n",
    "pickfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "44.13071084022522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#   Code to generate Samples. It fills 3 arrays.X(input),Y(positive output) and N(Negative samples)\n",
    "count = 0;\n",
    "dict_count = len(token_set)\n",
    "stop_words = stopwords.words('english')\n",
    "window = 3\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "t = time.time()\n",
    "X = []\n",
    "Y = []\n",
    "N = []\n",
    "for line in infile:\n",
    "    count += 1\n",
    "    if count == review_count:\n",
    "        break\n",
    "    jsondict = json.loads(line)\n",
    "    review_text = clean(jsondict[\"reviewText\"].lower())\n",
    "    tokens = tokenizer.tokenize(review_text)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token)<=2:\n",
    "            continue\n",
    "        token = preprocess(token)\n",
    "        if freq_map[token]<5:\n",
    "            continue\n",
    "        new_tokens.append(token)    \n",
    "    tokens = new_tokens   \n",
    "    for i in range(len(tokens)):\n",
    "        y_index = enc_map[tokens[i]]\n",
    "        x_out = np.zeros((dict_count,))\n",
    "        x_out[y_index] = 1\n",
    "        y_context = []\n",
    "        Neg = []\n",
    "        j_count = 0\n",
    "        for j in range(i-window,i+window+1):\n",
    "            if j!=i and j<len(tokens) and j>=0:\n",
    "                j_count+=1\n",
    "                x_temp = np.zeros((dict_count,))\n",
    "                x_index = enc_map[tokens[j]]\n",
    "                x_temp[x_index] = 1\n",
    "                y_context.append(x_index) \n",
    "        randnums = random.sample(range(len(final_list)),5)\n",
    "        for rand in randnums:\n",
    "            neg_word = final_list[rand]\n",
    "            neg_index = enc_map[neg_word]\n",
    "            n_temp = np.zeros((dict_count,))\n",
    "            n_temp[neg_index]=1\n",
    "            Neg.append(neg_index)        \n",
    "        Y.append(y_context)\n",
    "        X.append(y_index)\n",
    "        N.append(Neg)    \n",
    "        if(len(X)%50000==0):\n",
    "            print(\"50k samples generated\")\n",
    "infile.close()\n",
    "print(time.time()-t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856135\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(x,y,W1,W2):\n",
    "    h = W1[x]\n",
    "    u = np.dot(W2.T[y],h)\n",
    "    y_pred = sigmoid(u)\n",
    "    return h,u,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(alpha,epochs,encoder_length):\n",
    "    #print(time.time()-t1)\n",
    "    t = time.time()\n",
    "    W1 = np.random.uniform(-0.8,0.8,(dict_count,encoder_length))\n",
    "    W2 = np.random.uniform(-0.8,0.8,(encoder_length,dict_count))\n",
    "    dw2 = np.zeros(W2.shape)\n",
    "    dw1 = np.zeros(W1.shape)\n",
    "    for i in range(epochs):\n",
    "        loss = 0\n",
    "        p=0\n",
    "        for j in range(len(X)):\n",
    "            if j%100000 ==0:\n",
    "                print(\"100k done!\")\n",
    "                #print(\"t100\",time.time()-t)\n",
    "            p+=1\n",
    "            t0 = time.time()\n",
    "            x = X[j]\n",
    "            #print(x)\n",
    "            #dw2 = np.zeros(W2.shape)\n",
    "            dw1[x] = np.zeros(W1.shape[1])\n",
    "            t1 = time.time()\n",
    "\n",
    "            #for y in Y[j][0]:\n",
    "            for y in Y[j]:\n",
    "                h,u,y_pred = forwardProp(x,y,W1,W2)\n",
    "                loss += -np.log(y_pred)            \n",
    "                dw2.T[y] = h*(y_pred-1)\n",
    "                dw1[x] += W2.T[y]*(y_pred-1)\n",
    "            t2 = time.time()\n",
    "            N[j] = [n for n in N[j] if n not in Y[j]]\n",
    "            for n in N[j]:\n",
    "                h,u,y_pred = forwardProp(x,n,W1,W2)\n",
    "                #print(\"y\",y_pred)\n",
    "                loss += -np.log(1-y_pred)\n",
    "                dw2.T[n] = h*y_pred\n",
    "                dw1[x] += W2.T[n]*y_pred\n",
    "            for y in Y[j]:\n",
    "                W2.T[y] = W2.T[y]-alpha*dw2.T[y]\n",
    "            for n in N[j]:\n",
    "                W2.T[n] = W2.T[n]-alpha*dw2.T[n]\n",
    "            t4 = time.time()\n",
    "            W1[x] = W1[x]-alpha*dw1[x]\n",
    "            \n",
    "            t3 = time.time()\n",
    "        print(\"No. of iterations:{}, loss:{}\".format(i,loss))\n",
    "        print(time.time()-t)\n",
    "        if i%20==0:\n",
    "            pickfile = open(\"skipgram_parameters2\",\"wb\")\n",
    "            pickle.dump(W1,pickfile)\n",
    "    print(time.time()-t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:0, loss:6516530.128770456\n",
      "200.53109502792358\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:1, loss:5814537.697598275\n",
      "386.7836129665375\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:2, loss:5617898.560766266\n",
      "576.1239397525787\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:3, loss:5509288.117348146\n",
      "763.351655960083\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:4, loss:5437458.878578247\n",
      "950.6432898044586\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:5, loss:5384756.52051199\n",
      "1139.3589189052582\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:6, loss:5343468.567402897\n",
      "1328.01833486557\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:7, loss:5309682.103222957\n",
      "1522.2731988430023\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:8, loss:5281182.886065768\n",
      "1711.781280040741\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:9, loss:5256607.871283233\n",
      "1901.60236287117\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:10, loss:5235060.695064141\n",
      "2093.1876349449158\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:11, loss:5215920.494570154\n",
      "2282.7217967510223\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:12, loss:5198739.355309315\n",
      "2472.096759080887\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:13, loss:5183183.543185694\n",
      "2662.297273874283\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:14, loss:5168997.807773104\n",
      "2852.0416979789734\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:15, loss:5155982.612502134\n",
      "3040.826794862747\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:16, loss:5143979.037888136\n",
      "3229.5119829177856\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:17, loss:5132858.459400635\n",
      "3418.624948978424\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:18, loss:5122515.302476842\n",
      "3610.646795988083\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:19, loss:5112861.837629036\n",
      "3799.484263896942\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:20, loss:5103824.3638534695\n",
      "3987.305636882782\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:21, loss:5095340.35811035\n",
      "4175.960860967636\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:22, loss:5087356.310178256\n",
      "4364.506041765213\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:23, loss:5079826.05590947\n",
      "4553.850791931152\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:24, loss:5072709.481664112\n",
      "4743.804300069809\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:25, loss:5065971.506665313\n",
      "4932.471842050552\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:26, loss:5059581.271925654\n",
      "5123.560722827911\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:27, loss:5053511.483500919\n",
      "5315.00652384758\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:28, loss:5047737.874460433\n",
      "5503.571110963821\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:29, loss:5042238.761299267\n",
      "5693.665082931519\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:30, loss:5036994.676483313\n",
      "5882.503028869629\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:31, loss:5031988.062338244\n",
      "6070.792983055115\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:32, loss:5027203.014800086\n",
      "6260.361176967621\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:33, loss:5022625.068442548\n",
      "6450.933578014374\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:34, loss:5018241.016014746\n",
      "6640.2530307769775\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:35, loss:5014038.756626172\n",
      "6830.3378529548645\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:36, loss:5010007.167570892\n",
      "7022.107013940811\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:37, loss:5006135.995695363\n",
      "7213.302670717239\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:38, loss:5002415.765113113\n",
      "7403.94871711731\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:39, loss:4998837.698645718\n",
      "7593.766540050507\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:40, loss:4995393.650663279\n",
      "7783.462285995483\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:41, loss:4992076.049236677\n",
      "7973.86173582077\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:42, loss:4988877.845829383\n",
      "8163.575135946274\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:43, loss:4985792.471251913\n",
      "8353.330332994461\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:44, loss:4982813.796956675\n",
      "8542.567744016647\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:45, loss:4979936.100914847\n",
      "8731.175632953644\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:46, loss:4977154.037359008\n",
      "8919.543440818787\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:47, loss:4974462.609705527\n",
      "9112.59773683548\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:48, loss:4971857.145950204\n",
      "9301.074167013168\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:49, loss:4969333.276015321\n",
      "9490.316863059998\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:50, loss:4966886.910637459\n",
      "9677.950474023819\n",
      "100k done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:51, loss:4964514.221583117\n",
      "9866.208273887634\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:52, loss:4962211.623127158\n",
      "10054.496430158615\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:53, loss:4959975.754916333\n",
      "10243.161245822906\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:54, loss:4957803.466369726\n",
      "10432.199699878693\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:55, loss:4955691.802831041\n",
      "10621.477267026901\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:56, loss:4953637.993492473\n",
      "10812.045915842056\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:57, loss:4951639.440975208\n",
      "10999.957747936249\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:58, loss:4949693.712213494\n",
      "11188.695467948914\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:59, loss:4947798.530281189\n",
      "11377.495375871658\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:60, loss:4945951.766877095\n",
      "11565.10608792305\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:61, loss:4944151.435222087\n",
      "11752.697774887085\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:62, loss:4942395.683264789\n",
      "11940.490365982056\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:63, loss:4940682.786994534\n",
      "12128.746786117554\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:64, loss:4939011.143700878\n",
      "12315.376663923264\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:65, loss:4937379.2649810705\n",
      "12502.704514980316\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:66, loss:4935785.769372662\n",
      "12690.012900829315\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:67, loss:4934229.374539628\n",
      "12876.765909910202\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:68, loss:4932708.88905982\n",
      "13063.931329011917\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:69, loss:4931223.203926432\n",
      "13250.771421909332\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:70, loss:4929771.283960956\n",
      "13437.422640800476\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:71, loss:4928352.159326686\n",
      "13624.685540914536\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:72, loss:4926964.91735018\n",
      "13812.657299995422\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:73, loss:4925608.6947592\n",
      "13999.424911737442\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:74, loss:4924282.670441967\n",
      "14187.203207969666\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:75, loss:4922986.058721948\n",
      "14374.756265878677\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:76, loss:4921718.103195316\n",
      "14561.316847801208\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:77, loss:4920478.071069004\n",
      "14747.713525056839\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:78, loss:4919265.247994708\n",
      "14936.317718982697\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:79, loss:4918078.933339407\n",
      "15123.800196886063\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:80, loss:4916918.435832119\n",
      "15311.900855064392\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:81, loss:4915783.069568975\n",
      "15497.924263000488\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:82, loss:4914672.150419543\n",
      "15684.595190048218\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:83, loss:4913584.99300527\n",
      "15870.631198883057\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:84, loss:4912520.908438164\n",
      "16057.115203857422\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:85, loss:4911479.202981728\n",
      "16243.635189056396\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:86, loss:4910459.177734582\n",
      "16428.836885929108\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:87, loss:4909460.129261292\n",
      "16614.52105808258\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:88, loss:4908481.351116451\n",
      "16800.297434806824\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:89, loss:4907522.136094996\n",
      "16987.598289966583\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:90, loss:4906581.779135529\n",
      "17174.62836289406\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:91, loss:4905659.580726672\n",
      "17364.483278751373\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:92, loss:4904754.850745383\n",
      "17550.52210187912\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:93, loss:4903866.912619463\n",
      "17736.242115020752\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:94, loss:4902995.107665476\n",
      "17922.681921958923\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:95, loss:4902138.799503529\n",
      "18109.242985010147\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:96, loss:4901297.378347402\n",
      "18297.451282978058\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:97, loss:4900470.265042545\n",
      "18482.887607097626\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:98, loss:4899656.914660589\n",
      "18668.71218895912\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:99, loss:4898856.819521181\n",
      "18854.69441986084\n",
      "18854.694439888\n"
     ]
    }
   ],
   "source": [
    "training(0.01,100,100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(word):\n",
    "    pickfile = open(\"skipgram_parameters2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    return w1[enc_map[preprocess(word)]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14293598  0.37898354  0.46394955 -0.23686334  0.31166653  0.01384924\n",
      "  0.14757504  0.2822596   0.38947384 -0.02446139  0.08780087 -0.09145903\n",
      " -0.45241306  0.38050427  0.1270326   0.34816611  0.19654143  0.0313007\n",
      " -0.30355489  0.1289258   0.11602251  0.12763697  0.08636705 -0.275089\n",
      "  0.13530559  0.18536333 -0.17535636 -0.26797285  0.34005036 -0.19780731\n",
      "  0.37205776 -0.01342058 -0.38922718 -0.11636491  0.19380603 -0.2219386\n",
      " -0.11486295  0.17637688 -0.11576069 -0.1506918  -0.36205055  0.5418616\n",
      "  0.00252126 -0.31127348 -0.21496669  0.23703371  0.2658677  -0.12718166\n",
      "  0.21025002  0.07054467  0.54141329  0.2540037  -0.17323959 -0.28765072\n",
      "  0.25071124 -0.16504275 -0.04726763  0.1088582  -0.08353822  0.28568607\n",
      " -0.28060568  0.10028177 -0.12025568  0.36199659  0.92367953 -0.25173447\n",
      "  0.23636671  0.21262886 -0.19027988 -0.42581747 -0.14589755 -0.35792239\n",
      "  0.05878582 -0.03283561  0.04129483  0.02930252  0.05234729 -0.4186416\n",
      " -0.02196955 -0.36881438  0.18756035  0.18050571  0.69178774  0.07865867\n",
      "  0.10458415  0.4351116   0.33656106 -0.4680486   0.38891546  0.04583827\n",
      "  0.26779843 -0.21016556 -0.08082038 -0.18554953  0.32447775  0.07165285\n",
      "  0.53789993  0.19745871 -0.06808526 -0.15207564]\n"
     ]
    }
   ],
   "source": [
    "a = vector(\"camera\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word,count):\n",
    "    pickfile = open(\"skipgram_parameters2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    word_vec = w1[enc_map[preprocess(word)]]\n",
    "    similarity = {}\n",
    "    for i in range(len(token_set)):\n",
    "        new_vec = w1[i]\n",
    "        dot = np.dot(word_vec,new_vec)\n",
    "        magnitude = np.linalg.norm(new_vec)*np.linalg.norm(word_vec)\n",
    "        cosine_similarity = dot/magnitude\n",
    "        similarity[token_set[i]] = cosine_similarity\n",
    "     \n",
    "    similarity = sorted(similarity.items(),reverse=True,key = lambda similarity: (similarity[1],similarity[0]))\n",
    "    k =0\n",
    "    for wr,score in similarity:\n",
    "        if k==count:\n",
    "            break\n",
    "        print(wr,score)\n",
    "        k+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_vectors(word,count):\n",
    "    pickfile = open(\"skipgram_parameters2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    word_vec = w1[enc_map[preprocess(word)]]\n",
    "    similarity = {}\n",
    "    for i in range(len(token_set)):\n",
    "        new_vec = w1[i]\n",
    "        dot = np.dot(word_vec,new_vec)\n",
    "        magnitude = np.linalg.norm(new_vec)*np.linalg.norm(word_vec)\n",
    "        cosine_similarity = dot/magnitude\n",
    "        similarity[token_set[i]] = cosine_similarity\n",
    "     \n",
    "    similarity = sorted(similarity.items(),reverse=True,key = lambda similarity: (similarity[1],similarity[0]))\n",
    "    k =0\n",
    "    sim_vector = []\n",
    "    print(\"------- Similar words for {}-------------\".format(word))\n",
    "    for wr,score in similarity:\n",
    "        if k==count:\n",
    "            break\n",
    "        sim_vector.append(enc_map[wr])    \n",
    "        print(wr)\n",
    "        k+=1\n",
    "    return sim_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for camera on skipgram word2vec model using Negative Sampling:\n",
      "camera 1.0000000000000002\n",
      "len 0.6360120718961924\n",
      "and 0.5990720894993904\n",
      "this 0.5652517897859117\n",
      "you 0.545348721619646\n",
      "use 0.5410933308010849\n",
      "zoom 0.518922123192067\n",
      "one 0.5086358622892903\n",
      "with 0.5077737605433669\n",
      "canon 0.50069443367631\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar words for camera on skipgram word2vec model using Negative Sampling:\")\n",
    "similar_words('camera',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1302162   3.53775   ]\n",
      " [ 0.18294862 -1.0722537 ]\n",
      " [ 2.360141    6.3970227 ]\n",
      " ...\n",
      " [-7.358746    5.469925  ]\n",
      " [-5.97994     4.7740808 ]\n",
      " [ 2.0634887  -2.3136    ]]\n"
     ]
    }
   ],
   "source": [
    "w1 = load_model()\n",
    "embedding = TSNE(n_components=2).fit_transform(w1)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    pickfile = open(\"skipgram_parameters2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    wfile = open(\"dictionary_mappingsg2\",\"rb\")\n",
    "    pickfile.close()\n",
    "    wfile.close()\n",
    "    dict_map = pickle.load(wfile)\n",
    "    return w1,dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_embeddings(word,count):\n",
    "    sim_vectors = similar_vectors(word,count)\n",
    "    x = [embedding[i][0] for i in sim_vectors]\n",
    "    y = [embedding[i][1] for i in sim_vectors]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Similar words for music-------------\n",
      "music\n",
      "listen\n",
      "classic\n",
      "hear\n",
      "movi\n",
      "bass\n",
      "sound\n",
      "textbook\n",
      "record\n",
      "genr\n",
      "------- Similar words for noise-------------\n",
      "nois\n",
      "cancel\n",
      "ambient\n",
      "hiss\n",
      "sound\n",
      "block\n",
      "earbud\n",
      "volum\n",
      "loud\n",
      "bose\n",
      "------- Similar words for seeing-------------\n",
      "see\n",
      "say\n",
      "get\n",
      "imag\n",
      "jupit\n",
      "know\n",
      "photo\n",
      "give\n",
      "sure\n",
      "find\n",
      "------- Similar words for good-------------\n",
      "good\n",
      "great\n",
      "well\n",
      "decent\n",
      "veri\n",
      "excel\n",
      "nice\n",
      "better\n",
      "qualiti\n",
      "like\n",
      "------- Similar words for review-------------\n",
      "review\n",
      "peopl\n",
      "here\n",
      "post\n",
      "read\n",
      "rave\n",
      "articl\n",
      "research\n",
      "said\n",
      "sabrent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Word vectors using skipgram model')"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = sim_embeddings(\"music\",10)\n",
    "x1,y1 = sim_embeddings(\"noise\",10)\n",
    "x2,y2 = sim_embeddings(\"seeing\",10)\n",
    "x3,y3 = sim_embeddings(\"good\",10)\n",
    "x4,y4 = sim_embeddings(\"review\",10)\n",
    "xy = plt.scatter(x,y,color = \"blue\")\n",
    "x1y1 = plt.scatter(x1,y1,color=\"orange\")\n",
    "x2y2 = plt.scatter(x2,y2,color=\"green\")\n",
    "x3y3 = plt.scatter(x3,y3,color=\"red\")\n",
    "x4y4 = plt.scatter(x4,y4,color=\"pink\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"W1\")\n",
    "plt.ylabel(\"W2\")\n",
    "plt.legend((xy, x1y1, x2y2, x3y3, x4y4),\n",
    "           ('music', 'noise', 'seeing', 'good', 'review'),\n",
    "           scatterpoints=1,\n",
    "           loc='lower right',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n",
    "plt.title(\"Word vectors using skipgram model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
