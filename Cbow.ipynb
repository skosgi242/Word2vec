{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import time \n",
    "from sklearn import preprocessing as pre\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from Stemmer import Stemmer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict = dict()\n",
    "lemmatize_dict = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"/Users/skosgi/Downloads/nlp/\"\n",
    "\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "index_path = \"/Users/skosgi/Downloads/nlp/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    punctuations = '0123456789'\n",
    "    \n",
    "    d = str.maketrans(dict.fromkeys(punctuations,\" \"))\n",
    "    data = data.translate(d)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(token):\n",
    "    stemmer = Stemmer('english')\n",
    "    if token in stem_dict:\n",
    "        token = stem_dict[token]\n",
    "    else:\n",
    "        stem_word = stemmer.stemWord(token)\n",
    "        stem_dict[token] = stem_word\n",
    "        token = stem_word\n",
    "    if token in lemmatize_dict:\n",
    "        token = lemmatize_dict[token]\n",
    "    else:\n",
    "        lem_word = lemmatizer.lemmatize(token)\n",
    "        lemmatize_dict[token] = lem_word\n",
    "        token = lem_word\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_set 4940\n",
      "15.800692081451416\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "token_set = set()\n",
    "count = 0;\n",
    "stop_words = stopwords.words('english')\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "freq_map = {}\n",
    "for line in infile:\n",
    "    count += 1\n",
    "    if count == review_count:\n",
    "        break\n",
    "    jsondict = json.loads(line)\n",
    "    review_text = clean(jsondict[\"reviewText\"].lower())\n",
    "    tokens = tokenizer.tokenize(review_text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) <=2:\n",
    "            continue\n",
    "        token = preprocess(token)\n",
    "        token_set.add(token)\n",
    "        if token in freq_map.keys():\n",
    "            freq_map[token] = freq_map[token]+1\n",
    "        else:\n",
    "            freq_map[token] = 1\n",
    "token_set = list(token_set)\n",
    "token_set = [k for k in freq_map.keys() if freq_map[k]>=5]        \n",
    "print(\"token_set\",len(token_set))\n",
    "\n",
    "infile.close()\n",
    "enc_map = {}\n",
    "for i in range(len(token_set)):\n",
    "    enc_map[token_set[i]] = i\n",
    "final_list = []\n",
    "total_prob=0\n",
    "table_size = 100000000\n",
    "for key in token_set:\n",
    "    k = freq_map[key]\n",
    "    k = np.power(k,0.75)\n",
    "    total_prob +=k\n",
    "for key in token_set:\n",
    "    k = freq_map[key]\n",
    "    k = np.power(k,0.75)\n",
    "    pr_k = table_size*k/total_prob\n",
    "    for i in range(int(pr_k)):\n",
    "        final_list.append(key)\n",
    "        \n",
    "print(time.time()-t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickfile = open(\"dictionary_mappingcbow2\",\"wb\")\n",
    "pickle.dump(enc_map,pickfile)\n",
    "pickfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "50k samples generated\n",
      "54.691683769226074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#    lines,dict_count,enc_map=dictionary_count(review_count)\n",
    "count = 0;\n",
    "dict_count = len(token_set)\n",
    "stop_words = stopwords.words('english')\n",
    "window = 2\n",
    "infile = open(\"/Users/skosgi/Downloads/nlp/reviews_Electronics_5.json\",\"r\")\n",
    "t = time.time()\n",
    "X = []\n",
    "Y = []\n",
    "N = []\n",
    "for line in infile:\n",
    "    count += 1\n",
    "    if count == review_count:\n",
    "        break\n",
    "    jsondict = json.loads(line)\n",
    "    review_text = clean(jsondict[\"reviewText\"].lower())\n",
    "    tokens = tokenizer.tokenize(review_text)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token) <=2:\n",
    "            continue    \n",
    "        token = preprocess(token)\n",
    "        if(freq_map[token]<5):\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    tokens = [token for token in tokens if token in token_set]\n",
    "    for i in range(len(tokens)):\n",
    "        y_index = enc_map[tokens[i]]\n",
    "        x_context = []\n",
    "        Neg = []\n",
    "        j_count = 0\n",
    "        for j in range(i-window,i+window+1):\n",
    "            if j!=i and j<len(tokens) and j>=0:\n",
    "                j_count+=1\n",
    "                x_index = enc_map[tokens[j]]\n",
    "                x_context.append(x_index) \n",
    "        randnums = random.sample(range(len(final_list)),5)\n",
    "        for rand in randnums:\n",
    "            neg_word = final_list[rand]\n",
    "            neg_index = enc_map[neg_word]\n",
    "            Neg.append(neg_index)        \n",
    "        X.append(x_context)\n",
    "        Y.append(y_index)\n",
    "        N.append(Neg)    \n",
    "        if(len(X)%50000==0):\n",
    "            print(\"50k samples generated\")\n",
    "infile.close()\n",
    "print(time.time()-t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669631\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(u):\n",
    "    exp = np.exp(u-np.max(u))\n",
    "    exp = exp/exp.sum(axis=0)\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(x,y,W1,W2):\n",
    "    h = W1[x]\n",
    "    u = np.dot(W2.T[y],h)\n",
    "    y_pred = sigmoid(u)\n",
    "    return h,u,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(alpha,epochs,encoder_length):\n",
    "    #print(time.time()-t1)\n",
    "    t = time.time()\n",
    "    np.random.seed(3)\n",
    "    W1 = np.random.uniform(-0.8,0.8,(dict_count,encoder_length))\n",
    "    W2 = np.random.uniform(-0.8,0.8,(encoder_length,dict_count))\n",
    "    dw2 = np.zeros(W2.shape)\n",
    "    dw1 = np.zeros(W1.shape)\n",
    "    for i in range(epochs):\n",
    "        loss = 0\n",
    "        p=0\n",
    "        for j in range(len(X)):\n",
    "            if j%100000 ==0:\n",
    "                print(\"100k done!\")\n",
    "                #print(\"t100\",time.time()-t)\n",
    "            p+=1\n",
    "            t0 = time.time()              \n",
    "            y = Y[j]\n",
    "            \n",
    "            dw2.T[y] = np.zeros(W2.shape[0])\n",
    "            for n in N[j]:\n",
    "                dw2.T[n] = np.zeros(W2.shape[0])\n",
    "            for x in X[j]:\n",
    "                h,u,y_pred = forwardProp(x,y,W1,W2)\n",
    "                \"\"\"if j==3:\n",
    "                    print(\"y-pos\",y_pred)\"\"\"\n",
    "                loss += -np.log(y_pred)\n",
    "                dw1[x] = W2.T[y]*(y_pred-1)\n",
    "                dw2.T[y] = h*(y_pred-1)\n",
    "                for n in N[j]:\n",
    "                    h,u,y_pred = forwardProp(x,n,W1,W2)\n",
    "                    \"\"\"if j==2:\n",
    "                        print('y-neg',y_pred)\"\"\"\n",
    "                    loss += -np.log(1-y_pred)\n",
    "                    dw2.T[n] = h*y_pred\n",
    "                    dw1[x]+=W2.T[n]*y_pred\n",
    "                for n in N[j]:\n",
    "                    W2.T[n] = W2.T[n]-alpha*dw2.T[n]\n",
    "                #for x in X[j]:\n",
    "                W1[x] = W1[x]-alpha*dw1[x]\n",
    "                W2.T[y] = W2.T[y]-alpha*dw2.T[y]\n",
    "                \n",
    "            \n",
    "            #h,u,y_pred = forwardProp1(X[j],y,W1,W2)\n",
    "        print(\"No. of iterations:{}, loss:{}\".format(i,loss))\n",
    "        print(time.time()-t)\n",
    "        if i%20==0:\n",
    "            pickfile = open(\"cbow_params2\",\"wb\")\n",
    "            pickle.dump(W1,pickfile)\n",
    "    print(time.time()-t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:0, loss:6257546.9987917375\n",
      "312.32560992240906\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:1, loss:5344139.6139252735\n",
      "617.6572530269623\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:2, loss:5239643.467464705\n",
      "921.1434919834137\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:3, loss:5185793.02661498\n",
      "1226.4983789920807\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:4, loss:5149412.501605867\n",
      "1537.849436044693\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:5, loss:5121833.328066674\n",
      "1846.61692404747\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:6, loss:5099651.6825422915\n",
      "2152.947199821472\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:7, loss:5081174.150525599\n",
      "2459.748778820038\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:8, loss:5065417.410661506\n",
      "2766.377252101898\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:9, loss:5051749.967731921\n",
      "3072.3951489925385\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:10, loss:5039738.694040315\n",
      "3380.096739053726\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:11, loss:5029072.50306145\n",
      "3686.798229932785\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:12, loss:5019519.874461918\n",
      "3991.370589017868\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:13, loss:5010903.412539104\n",
      "4298.004902124405\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:14, loss:5003083.868520174\n",
      "4605.1895480155945\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:15, loss:4995949.722298699\n",
      "4912.617368936539\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:16, loss:4989410.171321563\n",
      "5221.092552185059\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:17, loss:4983390.29533985\n",
      "5527.593780994415\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:18, loss:4977827.636728438\n",
      "5833.723415851593\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:19, loss:4972669.700242036\n",
      "6140.413546800613\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:20, loss:4967872.055693174\n",
      "6448.246871948242\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:21, loss:4963396.854705877\n",
      "6756.448451042175\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:22, loss:4959211.651316471\n",
      "7069.0233290195465\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:23, loss:4955288.456857449\n",
      "7377.600692033768\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:24, loss:4951602.979562116\n",
      "7685.095774173737\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:25, loss:4948134.010659698\n",
      "7992.992818117142\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:26, loss:4944862.92643532\n",
      "8301.146674156189\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:27, loss:4941773.282414954\n",
      "8607.277740001678\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:28, loss:4938850.482191666\n",
      "8913.135138988495\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:29, loss:4936081.508137714\n",
      "9223.369066953659\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:30, loss:4933454.702654046\n",
      "9528.472783088684\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:31, loss:4930959.588534802\n",
      "9833.108110904694\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:32, loss:4928586.718679489\n",
      "10138.693951845169\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:33, loss:4926327.548488185\n",
      "10444.508225917816\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:34, loss:4924174.326841891\n",
      "10752.47611784935\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:35, loss:4922120.002788456\n",
      "11057.065320014954\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:36, loss:4920158.145553369\n",
      "11361.630974054337\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:37, loss:4918282.875712538\n",
      "11666.622809886932\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:38, loss:4916488.805668105\n",
      "11970.708536863327\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:39, loss:4914770.987875903\n",
      "12273.569440841675\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:40, loss:4913124.869727927\n",
      "12577.787240028381\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:41, loss:4911546.254302931\n",
      "12880.43674993515\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:42, loss:4910031.266174181\n",
      "13183.189980983734\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:43, loss:4908576.3214805955\n",
      "13485.437545061111\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:44, loss:4907178.101447724\n",
      "13788.583854913712\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:45, loss:4905833.528742284\n",
      "14092.287621974945\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:46, loss:4904539.746263738\n",
      "14395.119736909866\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:47, loss:4903294.098072727\n",
      "14697.24954199791\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:48, loss:4902094.11222157\n",
      "15001.509186983109\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:49, loss:4900937.485337391\n",
      "15304.978967905045\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:50, loss:4899822.068765224\n",
      "15607.642245054245\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:51, loss:4898745.856120945\n",
      "15909.25541806221\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:52, loss:4897706.972086144\n",
      "16211.47375202179\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:53, loss:4896703.662282006\n",
      "16512.148797035217\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:54, loss:4895734.28405448\n",
      "16814.24038386345\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:55, loss:4894797.29806388\n",
      "17117.567049980164\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:56, loss:4893891.260563243\n",
      "17422.83253097534\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:57, loss:4893014.816303031\n",
      "17725.112178087234\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100k done!\n",
      "No. of iterations:58, loss:4892166.691969552\n",
      "18027.706681013107\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:59, loss:4891345.690138559\n",
      "18330.012058019638\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:60, loss:4890550.683662287\n",
      "18630.84566807747\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:61, loss:4889780.6105043525\n",
      "18915.839801073074\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:62, loss:4889034.468973992\n",
      "19189.18202996254\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:63, loss:4888311.313351076\n",
      "19460.67697906494\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:64, loss:4887610.2498778645\n",
      "19732.2499229908\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:65, loss:4886930.433079231\n",
      "20000.79170703888\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:66, loss:4886271.062400231\n",
      "20270.037003993988\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:67, loss:4885631.379119561\n",
      "20539.61703801155\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:68, loss:4885010.663517566\n",
      "20809.845838069916\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:69, loss:4884408.2322696885\n",
      "21077.67474913597\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:70, loss:4883823.4360535545\n",
      "21349.78714299202\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:71, loss:4883255.657328069\n",
      "21621.35192990303\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:72, loss:4882704.308287935\n",
      "21891.42147707939\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:73, loss:4882168.828947237\n",
      "22161.685057878494\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:74, loss:4881648.685354721\n",
      "22432.16037797928\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:75, loss:4881143.367931055\n",
      "22702.634824991226\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:76, loss:4880652.389886706\n",
      "22971.867605924606\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:77, loss:4880175.285747101\n",
      "23240.150875091553\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:78, loss:4879711.609959783\n",
      "23510.197237968445\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:79, loss:4879260.935577153\n",
      "23779.973193883896\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:80, loss:4878822.853015165\n",
      "24049.5771920681\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:81, loss:4878396.968877733\n",
      "24316.394695997238\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:82, loss:4877982.904837571\n",
      "24584.29064798355\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:83, loss:4877580.296578062\n",
      "24853.756279945374\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:84, loss:4877188.792759396\n",
      "25121.128061056137\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:85, loss:4876808.054061106\n",
      "25387.34514093399\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:86, loss:4876437.752229492\n",
      "25654.51293206215\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:87, loss:4876077.569179735\n",
      "25921.045070886612\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:88, loss:4875727.196149646\n",
      "26189.45946598053\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:89, loss:4875386.332874464\n",
      "26460.741186857224\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:90, loss:4875054.686820315\n",
      "26727.646342992783\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:91, loss:4874731.972458681\n",
      "26994.634177923203\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:92, loss:4874417.910589337\n",
      "27261.790333032608\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:93, loss:4874112.22771489\n",
      "27528.58714199066\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:94, loss:4873814.655462661\n",
      "27795.072371006012\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:95, loss:4873524.930066536\n",
      "28065.679753780365\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:96, loss:4873242.79190372\n",
      "28335.028394937515\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:97, loss:4872967.985092218\n",
      "28604.707053899765\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:98, loss:4872700.257163388\n",
      "28871.254152059555\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "100k done!\n",
      "No. of iterations:99, loss:4872439.358790306\n",
      "29137.17639684677\n",
      "29137.176417827606\n"
     ]
    }
   ],
   "source": [
    "training(0.01,100,100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_words(word,count):\n",
    "    pickfile = open(\"cbow_params2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    wfile = open(\"dictionary_mappingcbow2\",\"rb\")\n",
    "    enc_map = pickle.load(wfile)\n",
    "    word_vec = w1[enc_map[preprocess(word)]]\n",
    "    similarity = {}\n",
    "    for i in range(len(token_set)):\n",
    "        new_vec = w1[i]\n",
    "        dot = np.dot(word_vec,new_vec)\n",
    "        magnitude = np.linalg.norm(new_vec)*np.linalg.norm(word_vec)\n",
    "        cosine_similarity = dot/magnitude\n",
    "        similarity[token_set[i]] = cosine_similarity\n",
    "     \n",
    "    similarity = sorted(similarity.items(),reverse=True,key = lambda similarity: (similarity[1],similarity[0]))\n",
    "    k =0\n",
    "    for wr,score in similarity:\n",
    "        if k==count:\n",
    "            break\n",
    "        print(wr,score)\n",
    "        k+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector(word):\n",
    "    pickfile = open(\"cbow_params2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    return w1[enc_map[preprocess(word)]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56965817, -0.68923911, -0.07708748,  0.25430846, -0.24154785,\n",
       "       -0.32172032,  0.22234619,  0.18904221,  0.44582496, -0.3025719 ,\n",
       "       -0.09298468, -0.29569578,  0.00867256, -0.22201751,  0.33997284,\n",
       "       -0.00930321, -0.22976571,  0.10554939, -0.35347581, -0.47479989,\n",
       "       -0.04603544,  0.52908996,  0.09820222, -0.20609834,  0.16607627,\n",
       "        0.06577984, -0.38663827,  0.39753055, -0.05377952, -0.29252249,\n",
       "       -0.458012  ,  0.19379599, -0.2812052 , -0.06976458, -0.00161043,\n",
       "        0.35926332, -0.07465457,  0.05603717, -0.25507639,  0.5304703 ,\n",
       "       -0.2503371 , -0.03168021,  0.18566935,  0.09794058,  0.76716876,\n",
       "       -0.10367375, -0.00169255,  0.13316833, -0.07537091, -0.05651939,\n",
       "       -0.20986527, -0.19494251, -0.09643766, -0.14612999, -0.4665728 ,\n",
       "       -0.05498664,  0.00281619, -0.31700697,  0.14444375,  0.42251368,\n",
       "        0.0986112 , -0.0231583 ,  0.0492561 ,  0.13306395,  0.24187035,\n",
       "       -0.36717307, -0.23710812, -0.35050261, -0.32852597,  0.09731138,\n",
       "        0.3252462 ,  0.15437631, -0.16880801,  0.51720427,  0.50172833,\n",
       "        0.14199073,  0.19268823, -0.59421481,  0.38140696, -0.54315475,\n",
       "       -0.41607692, -0.12602766,  0.50781343, -0.14094965, -0.33447009,\n",
       "       -0.33135471, -0.40653831, -0.27761271, -0.06812232,  0.20942909,\n",
       "       -0.0899839 ,  0.19170204,  0.00620696,  0.30672744,  0.42636895,\n",
       "       -0.02302721, -0.28370322, -0.07370379,  0.26530143,  0.15755654])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector(\"camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for camera on cbow word2vec model using Negative Sampling:\n",
      "camera 0.9999999999999998\n",
      "lens 0.6273957009136747\n",
      "it 0.5681831121263408\n",
      "unit 0.5595687826141541\n",
      "bag 0.5501650264361143\n",
      "one 0.5443173787403878\n",
      "flash 0.5352900862181103\n",
      "case 0.5342557273531074\n",
      "canon 0.526708159117382\n",
      "and 0.5157509624206492\n"
     ]
    }
   ],
   "source": [
    "print(\"Similar words for camera on cbow word2vec model using Negative Sampling:\")\n",
    "similar_words('camera',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_vectors(word,count):\n",
    "    pickfile = open(\"cbow_params2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    wfile = open(\"dictionary_mappingcbow2\",\"rb\")\n",
    "    enc_map = pickle.load(wfile)\n",
    "    word_vec = w1[enc_map[preprocess(word)]]\n",
    "    similarity = {}\n",
    "    for i in range(len(token_set)):\n",
    "        new_vec = w1[i]\n",
    "        dot = np.dot(word_vec,new_vec)\n",
    "        magnitude = np.linalg.norm(new_vec)*np.linalg.norm(word_vec)\n",
    "        cosine_similarity = dot/magnitude\n",
    "        similarity[token_set[i]] = cosine_similarity\n",
    "     \n",
    "    similarity = sorted(similarity.items(),reverse=True,key = lambda similarity: (similarity[1],similarity[0]))\n",
    "    k =0\n",
    "    sim_vector = []\n",
    "    print(\"--------Similar words for the word {}--------\".format(word))\n",
    "    for wr,score in similarity:\n",
    "        if k==count:\n",
    "            break\n",
    "        sim_vector.append(enc_map[wr])\n",
    "        print(wr)\n",
    "        k+=1\n",
    "    return sim_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81466395 -0.41194016]\n",
      " [-1.9940114  -1.8812935 ]\n",
      " [ 0.89110506  2.12647   ]\n",
      " ...\n",
      " [ 0.78499824  2.8141003 ]\n",
      " [ 1.1828004   0.4505266 ]\n",
      " [-0.01278617 -2.850783  ]]\n"
     ]
    }
   ],
   "source": [
    "pickfile = open(\"cbow_params2\",\"rb\")\n",
    "w1 = pickle.load(pickfile)\n",
    "embedding = TSNE(n_components=2).fit_transform(w1)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    pickfile = open(\"cbow_params2\",\"rb\")\n",
    "    w1 = pickle.load(pickfile)\n",
    "    wfile = open(\"dictionary_mappingcbow2\",\"rb\")\n",
    "    dict_map = pickle.load(wfile)\n",
    "    pickfile.close()\n",
    "    wfile.close()\n",
    "    model = {}\n",
    "    for key in dict_map.keys():\n",
    "        model[key] = w1[key]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_embeddings(word,count):\n",
    "    sim_vectors = similar_vectors(word,count)\n",
    "    x = [embedding[i][0] for i in sim_vectors]\n",
    "    y = [embedding[i][1] for i in sim_vectors]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Similar words for the word music--------\n",
      "music\n",
      "listen\n",
      "bass\n",
      "guitar\n",
      "ipod\n",
      "sound\n",
      "loud\n",
      "audio\n",
      "enjoy\n",
      "hear\n",
      "--------Similar words for the word noise--------\n",
      "nois\n",
      "elph\n",
      "quieter\n",
      "headach\n",
      "our\n",
      "mundan\n",
      "pul\n",
      "grasp\n",
      "daili\n",
      "jpeg\n",
      "--------Similar words for the word seeing--------\n",
      "see\n",
      "find\n",
      "read\n",
      "guess\n",
      "know\n",
      "tell\n",
      "hear\n",
      "get\n",
      "say\n",
      "look\n",
      "--------Similar words for the word good--------\n",
      "good\n",
      "great\n",
      "nice\n",
      "decent\n",
      "perfect\n",
      "cheap\n",
      "best\n",
      "sharp\n",
      "poor\n",
      "well\n",
      "--------Similar words for the word review--------\n",
      "review\n",
      "product\n",
      "here\n",
      "read\n",
      "item\n",
      "about\n",
      "nook\n",
      "model\n",
      "three\n",
      "amazon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Word vectors of words using cbow model')"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = sim_embeddings(\"music\",10)\n",
    "x1,y1 = sim_embeddings(\"noise\",10)\n",
    "x2,y2 = sim_embeddings(\"seeing\",10)\n",
    "x3,y3 = sim_embeddings(\"good\",10)\n",
    "x4,y4 = sim_embeddings(\"review\",10)\n",
    "xy = plt.scatter(x,y,color = \"blue\")\n",
    "x1y1 = plt.scatter(x1,y1,color=\"orange\")\n",
    "x2y2 = plt.scatter(x2,y2,color=\"green\")\n",
    "x3y3 = plt.scatter(x3,y3,color=\"red\")\n",
    "x4y4 = plt.scatter(x4,y4,color=\"pink\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"W1\")\n",
    "plt.ylabel(\"W2\")\n",
    "plt.legend((xy, x1y1, x2y2, x3y3, x4y4),\n",
    "           ('music', 'noise', 'seeing', 'good', 'review'),\n",
    "           scatterpoints=1,\n",
    "           loc='lower left',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n",
    "plt.title(\"Word vectors of words using cbow model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('/Users/skosgi/Downloads/GoogleNews-vectors-negative300.bin',binary=True,limit = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for camers on gensim word2vec model using googleNews data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cameras', 0.8131939172744751),\n",
       " ('camera_lens', 0.7250816822052002),\n",
       " ('camcorder', 0.7037475109100342),\n",
       " ('Camera', 0.6848659515380859),\n",
       " ('Cameras', 0.6350969076156616),\n",
       " ('tripod', 0.618983805179596),\n",
       " ('viewfinder', 0.596660852432251),\n",
       " ('webcam', 0.5858357548713684),\n",
       " ('Webcam', 0.5843442678451538),\n",
       " ('digital_cameras', 0.584299623966217)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Similar words for camers on gensim word2vec model using googleNews data\")\n",
    "model.most_similar(\"camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
